{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "import random\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe():\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8) \n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "        \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour():\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8) \n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]       \n",
    "        \n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                \n",
    "                if (\n",
    "                    r < 0\n",
    "                    or r >= self.row_count\n",
    "                    or c < 0\n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ): return i - 1\n",
    "                \n",
    "            return self.in_a_row - 1\n",
    "        \n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # Vertical win\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # Horizontal win\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # Top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # Top right diagonal\n",
    "        )\n",
    "        \n",
    "         \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "            \n",
    "        return policy, value\n",
    "\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0., -1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 1.],\n",
       "        [1., 0., 1.],\n",
       "        [1., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 1.]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2045/189462463.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_2.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8501967191696167 [0.00404677 0.33366236 0.04989177 0.08179387 0.00208749 0.05106269\n",
      " 0.39728817 0.07907131 0.00109554]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state = ttt.get_initial_state()\n",
    "state = ttt.get_next_state(state, 0, 1)\n",
    "state = ttt.get_next_state(state, 4, -1)\n",
    "state = ttt.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = ttt.get_encoded_state(state)\n",
    "\n",
    "display(state)\n",
    "display(encoded_state)\n",
    " \n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(ttt, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load('model_2.pt'))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value, policy)\n",
    "\n",
    "plt.bar(range(ttt.action_size), policy)\n",
    "# add title, y_axis label, and x_axis label to the plot\n",
    "plt.title('Policy (S)')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Action')\n",
    "# add annotation with the value in the top right part of the plot\n",
    "plt.annotate(f'V(S) = {value:.2f}', xy=(0.75, 0.75), xycoords='axes fraction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value) \n",
    "        \n",
    "            \n",
    "class AMCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, value = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # We add dirichlet noice to encourage exploration\n",
    "        policy = (1- self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        \n",
    "        policy /= np.sum(policy)\n",
    "        \n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)       \n",
    "        \n",
    "            node.backpropagate(value)\n",
    "        \n",
    "    \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "            \n",
    "        action_probs /= np.sum(action_probs)\n",
    "        \n",
    "        return action_probs\n",
    "\n",
    "        \n",
    "        #return visit_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                    \n",
    "                else:\n",
    "                    spg.node = node\n",
    "                    \n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "                    \n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "                \n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = AMCTS(game, args, model)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "             \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "             \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            \n",
    "            #Re-normalization\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "             \n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "             \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "             \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "             \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    \n",
    "                    returnMemory.append(\n",
    "                        (self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                 \n",
    "    \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample) # list for each instead of list of tuples\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            #Now we turn it to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            # For the policy loss we use multi-target cross entropy\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # We optimize the loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in trange(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "            \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"checkpoints_{self.game}/model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"checkpoints_{self.game}/optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = AMCTSParallel(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "        \n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            \n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "            \n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "                \n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "                    \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "        return return_memory\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "            \n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e01f6c1e5a45fa9dbf90c72cb44fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 39\u001b[0m\n\u001b[1;32m     25\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.30\u001b[39m\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     38\u001b[0m alphaZero \u001b[38;5;241m=\u001b[39m AlphaZeroParallel(model, optimizer, game, args)\n\u001b[0;32m---> 39\u001b[0m \u001b[43malphaZero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 79\u001b[0m, in \u001b[0;36mAlphaZeroParallel.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selfPlay_iteration \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_selfPlay_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_parallel_games\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 79\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfPlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[69], line 18\u001b[0m, in \u001b[0;36mAlphaZeroParallel.selfPlay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([spg\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m spg \u001b[38;5;129;01min\u001b[39;00m spGames])\n\u001b[1;32m     16\u001b[0m neutral_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mchange_perspective(states, player)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneutral_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspGames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(spGames))[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     spg \u001b[38;5;241m=\u001b[39m spGames[i]\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 47\u001b[0m, in \u001b[0;36mAMCTSParallel.search\u001b[0;34m(self, states, spGames)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expandable_spGames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([spGames[mappingIdx]\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m mappingIdx \u001b[38;5;129;01min\u001b[39;00m expandable_spGames])\n\u001b[0;32m---> 47\u001b[0m     policy, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoded_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     policy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(policy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     51\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 37\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstartBlock(x)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resBlock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackBone:\n\u001b[0;32m---> 37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mresBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicyHead(x)\n\u001b[1;32m     39\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalueHead(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 54\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     56\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/style/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Only run this to train the model\n",
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# model = ResNet(game, 4, 64, device=device)\n",
    "model = ResNet(game, 9, 128, device=device) # Use for connect fout\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "# args = {\n",
    "#     'C': 2, \n",
    "#     'num_searches': 60,\n",
    "#     'num_iterations': 3,\n",
    "#     'num_selfPlay_iterations': 500,\n",
    "#     'num_epochs': 4,\n",
    "#     'batch_size': 64,\n",
    "#     'temperature': 1.25,\n",
    "#     'dirichlet_epsilon': 0.25,\n",
    "#     'dirichlet_alpha': 0.30\n",
    "# }\n",
    "\n",
    "\n",
    "args = {\n",
    "    'C': 2, \n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'num_parallel_games': 100,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.30\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0. -1.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  0.  0. -1.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  0.  0. -1.  0.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  0.  0. -1.  0.  1.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  0.  0. -1.  0.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  0.  0. -1.  0.  1.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  1.  0. -1.  0.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [-1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  1.  0. -1.  0.  1.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [-1.  0.  0.  0. -1.  0.  0.]\n",
      " [ 1.  1.  1.  1. -1.  0.  1.]]\n",
      "1 won\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2, \n",
    "    'num_searches': 100,\n",
    "    'dirichlet_epsilon': 0.0,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load model from checkpoints\n",
    "checkpoint_path = '/home/haunted/Courses/opti/alpha-zero/model_2.pt'\n",
    "\n",
    "model = ResNet(game, 9, 128, device=device)\n",
    "\n",
    "# Load the state dictionary from the checkpoint\n",
    "#state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "#model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "mcts = AMCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}\"))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "    \n",
    "    else: \n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "    \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "    \n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "state = game.get_initial_state()\n",
    "game.get_valid_moves(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m prob \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1693325\u001b[39m,  \u001b[38;5;241m0.23998272\u001b[39m, \u001b[38;5;241m0.10992078\u001b[39m, \u001b[38;5;241m0.1209937\u001b[39m,  \u001b[38;5;241m0.10676885\u001b[39m, \u001b[38;5;241m0.11384467\u001b[39m,\n\u001b[1;32m      2\u001b[0m  \u001b[38;5;241m0.13915679\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mprob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "prob = [0.1693325,  0.23998272, 0.10992078, 0.1209937,  0.10676885, 0.11384467,\n",
    " 0.13915679]\n",
    "\n",
    "prob > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style",
   "language": "python",
   "name": "style"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
